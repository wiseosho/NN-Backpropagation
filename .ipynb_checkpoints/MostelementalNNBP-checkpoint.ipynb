{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mnist_loader\n",
    "~~~~~~~~~~~~\n",
    "\n",
    "A library to load the MNIST image data.  For details of the data\n",
    "structures that are returned, see the doc strings for ``load_data``\n",
    "and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n",
    "function usually called by our neural network code.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import _pickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('./data/mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = _pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = [ [inputs, results] for inputs, results in zip(training_inputs, training_results) ] # zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = [ [ va_i, va_o   ] for va_i, va_o in zip(validation_inputs, va_d[1])]#zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = [ [te_i, te_o] for te_i, te_o in zip(test_inputs, te_d[1]) ]#zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training, validation, test = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(training)\n",
    "np.shape(training[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "\n",
    "def P_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "def Cost_der(h, y):\n",
    "    return (h-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \n",
    "    def __init__(self, net_dim, lr=0.1, Ws=None, Bs=None): # net_dim = [#nodes layer, start from input nodes]\n",
    "        Nlayer = len(net_dim)\n",
    "        if (weights == None) or (biases == None):\n",
    "            self.weights = [np.random.randn(o, i) for i, o in zip(net_dim[:-1], net_dim[1:] )]\n",
    "            self.biases = [np.random.randn(o, 1) for o in net_dim[1:] ]\n",
    "        \n",
    "        else:\n",
    "            self.weights = Ws\n",
    "            self.biases = Bs\n",
    "        self.lr = lr\n",
    "        #self.activations = np.zeros((Nlayer, ))\n",
    "        #self.output = \n",
    "        \n",
    "    def stochastic_gradient_descent(self,train, epochs, test=None, lr=0.5, Bsize = 200):\n",
    "        # Mini_batch with k partitioning will be used\n",
    "        N = len(training)\n",
    "        idx = np.arange(0, N, 1)\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            #devide train_data_set\n",
    "            #np.random.shuffle(train)\n",
    "            #np.arange(0, N, partition)\n",
    "            #batches = [ train[i : i+k] for i in range(0, N, Bsize)]\n",
    "\n",
    "            np.random.shuffle(training)\n",
    "            #training_shuffled = [ [training[0][i],training[1][i]] for i in idx]\n",
    "            batches = [ training[i: i+Bsize] for i in range(0, N, Bsize)]\n",
    "\n",
    "            #Learning from the batch data\n",
    "            for batch in batches:\n",
    "                self.update_from_batch(batch)\n",
    "\n",
    "            # Evaluate from the test data\n",
    "            if test :\n",
    "                print (\"Epoch {0} : {1} accuracy\".format(epoch, self.evaluate(test)) )\n",
    "            else:\n",
    "                print (\"Epoch {0} Complete. No test data available\")\n",
    "        return (self.weights, self.biases)\n",
    "\n",
    "    def update_from_batch(self,batch):\n",
    "        #batch is composed of x, y of input node, labels\n",
    "        N = len(batch)\n",
    "        #make placeholders for delta_parameters\n",
    "        accum_delta_w = [np.zeros_like(w) for w in self.weights]\n",
    "        accum_delta_b = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        #calculate backpropagated derivatives of Cost function w.r.t weights and biases\n",
    "        # This \n",
    "        for sample in batch: # Suppose the sample consists of [input, label], where shape(input) = n,1/\n",
    "            del_w, del_b = self.backpropagation(sample) # accum_delta_w, accum_delta_b #\n",
    "        \n",
    "            #add calculated sample into (accumulate)\n",
    "            accum_delta_w = [acc_del_w + del_w for acc_del_w, del_w in zip(accum_delta_w,del_w )]\n",
    "            accum_delta_b = [acc_del_b + del_b for acc_del_b, del_b in zip(accum_delta_b, del_b)]\n",
    "\n",
    "        # weight correction using accumulated delta with learning rate(lr)\n",
    "        self.weights = [w - (self.lr/N)*del_w for w, del_w in zip(self.weights, accum_delta_w)]\n",
    "        self.biases = [b - (self.lr/N)*del_b for b, del_b in zip(self.biases, accum_delta_b)]\n",
    "    def backpropagation(self, sample):\n",
    "        #Suppose sample is One case with N feasures in shape of matrix(N,1)\n",
    "        #activations and sum values of each layer should be counted and listed in proccess of feedforward \n",
    "        #\n",
    "        #Layer_val = np.zeros(1,1)\n",
    "        #Storing intermediate output values will be appended from existing list\n",
    "        in_a = [sample[0]] #first value will be the input features\n",
    "        out_z = []\n",
    "        ina = copy.deepcopy(sample[0])\n",
    "        #Feed_forward and save each layer output\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, ina)+b\n",
    "            ina = sigmoid(z)\n",
    "            out_z.append(z)\n",
    "            in_a.append(ina)\n",
    "        #Backpropagation using parameters\n",
    "        dw = [np.zeros_like(w) for w in self.weights]\n",
    "        db = [np.zeros_like(b) for b in self.biases]\n",
    "        N = len(out_z)\n",
    "        #1. Get Cost derivative values(dz, dw, db) of the Last layer\n",
    "        dz = Cost_der(in_a[-1], sample[1])* P_sigmoid(out_z[-1]) #(dz is the delta value)\n",
    "        dw[-1] = in_a[-2].transpose()*dz # (1,N) * (N,1)\n",
    "        db[-1] = dz #(N,1)\n",
    "        #2. Get iterative, # for i the iterative (from last layerN)\n",
    "        #a0 --- (z0)a1 --- (z1)a2 ---(z2) : a3 --- C\n",
    "        #    w0         w1         w2\n",
    "        for i in range(N-2, -1,-1) : \n",
    "            dz = np.dot(np.transpose(self.weights[i+1]),dz) * P_sigmoid(out_z[i])\n",
    "            dw[i] = in_a[i].transpose()*dz # (1,N) * (N,1)\n",
    "            db[i] = dz #(N,1)\n",
    "        #3. \n",
    "        return (dw, db)\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        a=[]\n",
    "        \n",
    "        for in_a, label in X:\n",
    "            ina = copy.deepcopy(in_a)\n",
    "            #weights and biases are yet seperated.\n",
    "            for w, b in zip(self.weights, self.biases):\n",
    "                ina = sigmoid(np.dot(w, ina) + b)\n",
    "            a.append(np.argmax(ina) == label)\n",
    "        return (np.array(a))\n",
    "\n",
    "\n",
    "    def evaluate(self, test):\n",
    "        #Return Onehot Encoding | Assume x:0, y:1\n",
    "        return (np.sum( self.feedforward(test))/len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(784, 10, 10)\n",
    "net_dim = [784, 10, 10]\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 0.1744 accuracy\n",
      "Epoch 0 : 0.1923 accuracy\n",
      "Epoch 1 : 0.2065 accuracy\n",
      "Epoch 0 : 0.2217 accuracy\n",
      "Epoch 1 : 0.2441 accuracy\n",
      "Epoch 2 : 0.2711 accuracy\n",
      "Epoch 0 : 0.2977 accuracy\n",
      "Epoch 1 : 0.3396 accuracy\n",
      "Epoch 2 : 0.3585 accuracy\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    nn = Network(net_dim , Ws=weights, Bs = biases)\n",
    "    weights, biases = nn.stochastic_gradient_descent(training, i, test=test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net_dim = [784, 10,10]\n",
    "\n",
    "weights =  [ np.random.randn(o,i) for o, i in zip(net_dim[1:] , net_dim[:-1])]\n",
    "biases = [np.random.randn(o,1) for o in net_dim[1:]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backpropagation(sample, weights, biases): #Cal for One sample case\n",
    "\n",
    "    #Suppose sample is One case with N feasures in shape of matrix(N,1)\n",
    "    #activations and sum values of each layer should be counted and listed in proccess of feedforward \n",
    "    #\n",
    "    #Layer_val = np.zeros(1,1)\n",
    "    #Storing intermediate output values will be appended from existing list\n",
    "    #N = \n",
    "    in_a = [sample[0]] #first value will be the input features\n",
    "    out_z = []\n",
    "    \n",
    "    #Feed_forward and save each layer output\n",
    "    for w, b in zip(weights, biases):\n",
    "        z = np.dot(w, in_a[-1])+b\n",
    "        a = sigmoid(z)\n",
    "        out_z.append(z)\n",
    "        in_a.append(a)\n",
    "    #Backpropagation using parameters\n",
    "    #1. Get Cost derivative values(dz, dw, db) of the Last layer\n",
    "    dw = [np.zeros_like(w) for w in weights]\n",
    "    db = [np.zeros_like(b) for b in biases]\n",
    "    N = len(out_z)\n",
    "    dz = Cost_der(in_a[-1], sample[1])* P_sigmoid(out_z[-1]) #(dz is the delta value)\n",
    "    dw[-1] = in_a[-2].transpose()*dz # (1,N) * (N,1)\n",
    "    db[-1] = dz #(N,1)\n",
    "    #2. Get iterative, # for i the iterative (from last layerN)\n",
    "    #a0 --- (z0)a1 --- (z1)a2 ---(z2) : a3 --- C\n",
    "    #    w0         w1         w2\n",
    "    for i in range(N-2, -1,-1) : \n",
    "        dz = np.dot(np.transpose(weights[i+1]),dz) * P_sigmoid(z[i])\n",
    "        dw[i] = in_a[i].transpose()*dz # (1,N) * (N,1)\n",
    "        db[i] = dz #(N,1)\n",
    "\n",
    "    #3.\n",
    "    return (dw, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dw = [np.zeros_like(w) for w in weights]\n",
    "db = [np.zeros_like(b) for b in biases]\n",
    "batch_in = training[0][:200]\n",
    "batch_lbl= np.array(training[1][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for inp, out in zip(batch_in, batch_lbl):\n",
    "    Dw, Db = backpropagation([inp,out], weights, biases)\n",
    "    dw = [ dw + Dw for dw, Dw in zip(dw, Dw)]\n",
    "    db = [ db + Db for db, Db in zip(db, Db)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0045748028676951"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(dw[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 176,\n",
       "        177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189,\n",
       "        190, 191, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213,\n",
       "        214, 215, 216, 217, 218, 231, 232, 233, 234, 235, 236, 237, 238,\n",
       "        239, 240, 241, 260, 261, 262, 263, 264, 265, 266, 268, 269, 289,\n",
       "        290, 291, 292, 293, 319, 320, 321, 322, 347, 348, 349, 350, 376,\n",
       "        377, 378, 379, 380, 381, 405, 406, 407, 408, 409, 410, 434, 435,\n",
       "        436, 437, 438, 439, 463, 464, 465, 466, 467, 493, 494, 495, 496,\n",
       "        518, 519, 520, 521, 522, 523, 524, 544, 545, 546, 547, 548, 549,\n",
       "        550, 551, 570, 571, 572, 573, 574, 575, 576, 577, 578, 596, 597,\n",
       "        598, 599, 600, 601, 602, 603, 604, 605, 622, 623, 624, 625, 626,\n",
       "        627, 628, 629, 630, 631, 648, 649, 650, 651, 652, 653, 654, 655,\n",
       "        656, 657, 676, 677, 678, 679, 680, 681, 682, 683]),)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(dw[0][0])\n",
    "np.where(dw[0][0] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dz = Cost_der(As[-1], training[1][0])*P_sigmoid(Zs[-1])\n",
    "dw[-1] = As[-2].transpose()*dz\n",
    "db[-1] = dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [-0., -0., -0., ..., -0., -0., -0.],\n",
      "       ..., \n",
      "       [-0., -0., -0., ..., -0., -0., -0.],\n",
      "       [-0., -0., -0., ..., -0., -0., -0.],\n",
      "       [-0., -0., -0., ..., -0., -0., -0.]]), array([[  1.39291766e-01,   2.29537404e-03,   1.76381838e-02,\n",
      "          1.39561404e-01,   7.72245085e-02,   1.39746121e-01,\n",
      "          1.34929086e-01,   4.51977709e-03,   1.26357874e-01,\n",
      "          3.63729785e-07],\n",
      "       [  3.45410781e-02,   5.69198712e-04,   4.37385425e-03,\n",
      "          3.46079420e-02,   1.91498597e-02,   3.46537473e-02,\n",
      "          3.34592360e-02,   1.12079829e-03,   3.13337775e-02,\n",
      "          9.01964219e-08],\n",
      "       [  9.25102919e-02,   1.52446715e-03,   1.17143574e-02,\n",
      "          9.26893714e-02,   5.12884718e-02,   9.28120503e-02,\n",
      "          8.96128281e-02,   3.00179910e-03,   8.39202787e-02,\n",
      "          2.41570263e-07],\n",
      "       [  1.46870319e-01,   2.42026020e-03,   1.85978379e-02,\n",
      "          1.47154628e-01,   8.14261212e-02,   1.47349394e-01,\n",
      "          1.42270275e-01,   4.76568805e-03,   1.33232724e-01,\n",
      "          3.83519509e-07],\n",
      "       [  1.18764604e-01,   1.95710915e-03,   1.50388783e-02,\n",
      "          1.18994506e-01,   6.58440800e-02,   1.19152001e-01,\n",
      "          1.15044844e-01,   3.85370616e-03,   1.07736756e-01,\n",
      "          3.10127621e-07],\n",
      "       [ -3.11419752e-02,  -5.13185262e-04,  -3.94343398e-03,\n",
      "         -3.12022592e-02,  -1.72653689e-02,  -3.12435569e-02,\n",
      "         -3.01665946e-02,  -1.01050328e-03,  -2.82502972e-02,\n",
      "         -8.13204131e-08],\n",
      "       [  3.02047846e-02,   4.97741398e-04,   3.82475977e-03,\n",
      "          3.02632544e-02,   1.67457827e-02,   3.03033093e-02,\n",
      "          2.92587572e-02,   9.80093061e-04,   2.74001291e-02,\n",
      "          7.88731461e-08],\n",
      "       [  5.18083706e-02,   8.53744570e-04,   6.56037029e-03,\n",
      "          5.19086602e-02,   2.87229896e-02,   5.19773638e-02,\n",
      "          5.01857092e-02,   1.68109209e-03,   4.69977212e-02,\n",
      "          1.35286155e-07],\n",
      "       [  1.47202770e-01,   2.42573862e-03,   1.86399353e-02,\n",
      "          1.47487722e-01,   8.16104345e-02,   1.47682929e-01,\n",
      "          1.42592313e-01,   4.77647549e-03,   1.33534304e-01,\n",
      "          3.84387630e-07],\n",
      "       [  1.75882349e-02,   2.89834632e-04,   2.22715619e-03,\n",
      "          1.76222818e-02,   9.75106304e-03,   1.76456058e-02,\n",
      "          1.70373635e-02,   5.70707826e-04,   1.59550850e-02,\n",
      "          4.59278037e-08]])] [array([[  9.15993271e-04],\n",
      "       [  4.54633165e-03],\n",
      "       [ -2.06240873e-02],\n",
      "       [  7.52336281e-04],\n",
      "       [  1.63813479e-02],\n",
      "       [  8.66168504e-04],\n",
      "       [ -7.02813874e-03],\n",
      "       [ -8.30069343e-03],\n",
      "       [ -9.58536035e-04],\n",
      "       [ -6.29205545e-08]]), array([[ 0.14016976],\n",
      "       [ 0.0347588 ],\n",
      "       [ 0.09309341],\n",
      "       [ 0.14779609],\n",
      "       [ 0.11951321],\n",
      "       [-0.03133827],\n",
      "       [ 0.03039517],\n",
      "       [ 0.05213493],\n",
      "       [ 0.14813063],\n",
      "       [ 0.0176991 ]])]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Zs)-2, -1,-1):\n",
    "    print(i)\n",
    "    dz = np.dot(weights[i+1].transpose(), dz) * P_sigmoid(Zs[i])\n",
    "    dw[i] = As[i].transpose()*dz\n",
    "    db[i] = dz\n",
    "\n",
    "print (dw, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(weights[0].transpose())\n",
    "\n",
    "np.shape(Zs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(training[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.22457135e-03],\n",
       "       [  1.61075093e-02],\n",
       "       [  1.10000135e-01],\n",
       "       [  4.32131601e-03],\n",
       "       [  2.47405567e-01],\n",
       "       [  3.01321376e-03],\n",
       "       [  3.59902021e-02],\n",
       "       [  3.12052807e-02],\n",
       "       [  8.88273462e-02],\n",
       "       [  2.59491659e-06]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_sigmoid(Zs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 10)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(weights[0].transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shuffled'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1e4adc22b54f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shuffled'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.0078125 ],\n",
       "       [ 0.359375  ],\n",
       "       [ 0.74609375],\n",
       "       [ 0.62890625],\n",
       "       [ 0.99609375],\n",
       "       [ 0.61328125],\n",
       "       [ 0.35546875],\n",
       "       [ 0.0078125 ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.1953125 ],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.4296875 ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.01171875],\n",
       "       [ 0.6796875 ],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.83984375],\n",
       "       [ 0.08203125],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.43359375],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.14453125],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.015625  ],\n",
       "       [ 0.671875  ],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.83984375],\n",
       "       [ 0.51171875],\n",
       "       [ 0.96484375],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.14453125],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.125     ],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.88671875],\n",
       "       [ 0.40234375],\n",
       "       [ 0.        ],\n",
       "       [ 0.77734375],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.14453125],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.125     ],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.95703125],\n",
       "       [ 0.23828125],\n",
       "       [ 0.        ],\n",
       "       [ 0.1796875 ],\n",
       "       [ 0.91796875],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.14453125],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.09765625],\n",
       "       [ 0.91796875],\n",
       "       [ 0.77734375],\n",
       "       [ 0.25390625],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.26953125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.921875  ],\n",
       "       [ 0.11328125],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.18359375],\n",
       "       [ 0.09765625],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.375     ],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.54296875],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.75      ],\n",
       "       [ 0.98828125],\n",
       "       [ 0.91015625],\n",
       "       [ 0.125     ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.75      ],\n",
       "       [ 0.98828125],\n",
       "       [ 0.671875  ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.19140625],\n",
       "       [ 0.9375    ],\n",
       "       [ 0.98828125],\n",
       "       [ 0.53125   ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.3984375 ],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.19140625],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.7265625 ],\n",
       "       [ 0.98828125],\n",
       "       [ 0.578125  ],\n",
       "       [ 0.03515625],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.3671875 ],\n",
       "       [ 0.953125  ],\n",
       "       [ 0.98828125],\n",
       "       [ 0.21484375],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.03125   ],\n",
       "       [ 0.4375    ],\n",
       "       [ 0.87109375],\n",
       "       [ 0.80078125],\n",
       "       [ 0.390625  ],\n",
       "       [ 0.390625  ],\n",
       "       [ 0.81640625],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.21484375],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.34375   ],\n",
       "       [ 0.671875  ],\n",
       "       [ 0.02734375],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.51171875],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.53515625],\n",
       "       [ 0.4140625 ],\n",
       "       [ 0.4140625 ],\n",
       "       [ 0.4140625 ],\n",
       "       [ 0.875     ],\n",
       "       [ 0.98828125],\n",
       "       [ 0.0703125 ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.55859375],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.51953125],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.046875  ],\n",
       "       [ 0.96875   ],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.76171875],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.55078125],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.05078125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.69140625],\n",
       "       [ 0.8046875 ],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.75      ],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.98828125],\n",
       "       [ 0.55078125],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "l= [[1,2],[3,4]]\n",
    "for i, j in l:\n",
    "    print (i+ j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
